{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from idavatar.unet import UNetModel\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from transformers import CLIPTextModel\n",
    "\n",
    "from transformers.models.clip.modeling_clip import (\n",
    "    _expand_mask,\n",
    "    CLIPTextTransformer,\n",
    "    CLIPPreTrainedModel,\n",
    "    CLIPModel,\n",
    ")\n",
    "\n",
    "model_path = 'D:/apps/tools/programming/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/9cd4afd23ecbf0348e2c46f4ac712dbf032da73c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 77, 768]), torch.Size([2, 768]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IDAvatarTextEncoder(CLIPPreTrainedModel):\n",
    "    _build_causal_attention_mask = CLIPTextTransformer._build_causal_attention_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(model_name_or_path, **kwargs):\n",
    "        model = CLIPTextModel.from_pretrained(model_name_or_path, **kwargs)\n",
    "        text_model = model.text_model\n",
    "        return IDAvatarTextEncoder(text_model)\n",
    "\n",
    "    def __init__(self, text_model):\n",
    "        super().__init__(text_model.config)\n",
    "        self.config = text_model.config\n",
    "        self.final_layer_norm = text_model.final_layer_norm\n",
    "        self.embeddings = text_model.embeddings\n",
    "        self.encoder = text_model.encoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        use_causual_mask=True,\n",
    "    ):\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "\n",
    "        if use_causual_mask:\n",
    "            causal_attention_mask = self._build_causal_attention_mask(\n",
    "                                        bsz, seq_len, hidden_states.dtype\n",
    "                                    ).to(self.device)\n",
    "            \n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(\n",
    "                dim=-1\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "    \n",
    "text_encoder = IDAvatarTextEncoder.from_pretrained(model_path, subfolder='text_encoder')\n",
    "input_ids = torch.randint(low=1, high=400, size=(2, 77))\n",
    "a, b = text_encoder(input_ids)\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 768]), torch.Size([2, 256, 1024]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "import torch.nn as nn\n",
    "# clip_path = 'D:/apps/tools/programming/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/8d052a0f05efbaefbc9e8786ba291cfdf93e5bff'\n",
    "\n",
    "class IDAvatarCLIPImageEncoder(nn.Module):\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        global_model_name_or_path,\n",
    "    ):\n",
    "        model,_,_ = open_clip.create_model_and_transforms('ViT-L-14',global_model_name_or_path)\n",
    "        vision_model = model.visual\n",
    "        vision_model.output_tokens = True\n",
    "        vision_processor = T.Normalize(\n",
    "            (0.48145466, 0.4578275, 0.40821073),\n",
    "            (0.26862954, 0.26130258, 0.27577711),\n",
    "        )\n",
    "        return IDAvatarCLIPImageEncoder(\n",
    "            vision_model,\n",
    "            vision_processor,\n",
    "        )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model,\n",
    "        vision_processor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.vision_processor = vision_processor\n",
    "\n",
    "        self.image_size = vision_model.image_size\n",
    "\n",
    "    def forward(self, person_pixel_values):\n",
    "        b, c, h, w = person_pixel_values.shape\n",
    "\n",
    "        if (h, w) != self.image_size:\n",
    "            h, w = self.image_size\n",
    "            person_pixel_values = F.interpolate(\n",
    "                person_pixel_values, (h, w), mode=\"bilinear\", antialias=True\n",
    "            )# b, c, h, w -> b, c, 224, 224\n",
    "        person_pixel_values = self.vision_processor(person_pixel_values) \n",
    "        person_embeds, patch_features = self.vision_model(person_pixel_values)# b, 1048; b, 256, 1280\n",
    "        person_embeds = person_embeds.view(b, 1, -1) # b, 1, 1280\n",
    "        return person_embeds, patch_features\n",
    "    \n",
    "image_encoder = IDAvatarCLIPImageEncoder.from_pretrained('datacomp_xl_s13b_b90k')\n",
    "images = torch.rand((2, 3, 256, 256))\n",
    "a, b = image_encoder(images)\n",
    "a.shape, b.shape\n",
    "# image_encoder.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024]), torch.Size([1024]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder(images)[0].shape,image_encoder(images)[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
